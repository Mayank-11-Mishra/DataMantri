# ğŸ‰ Complete Pipeline Orchestrator - READY TO USE!

## âœ… What's Been Built

### **Backend (FastAPI + Celery)**
- âœ… User authentication (JWT)
- âœ… Pipeline CRUD APIs
- âœ… Pipeline execution engine
- âœ… BigQuery to PostgreSQL transfer
- âœ… Celery async tasks
- âœ… Scheduler with Celery Beat
- âœ… PostgreSQL metadata storage
- âœ… Redis for task queue

### **Frontend (React + TypeScript)**
- âœ… Login page
- âœ… Dashboard with stats
- âœ… Pipelines list & management
- âœ… Create pipeline form
- âœ… Pipeline detail & execution history
- âœ… Real-time status updates
- âœ… Modern Tailwind UI

### **Infrastructure**
- âœ… Docker Compose setup
- âœ… PostgreSQL database
- âœ… Redis cache/queue
- âœ… Auto-initialization scripts
- âœ… QUICKSTART automation

---

## ğŸš€ How to Start Everything

### **Option 1: Automated Start (Recommended)**

```bash
cd "/Users/sunny.agarwal/Projects/DataMantri - Cursor/pipeline_orchestrator"

# Make script executable
chmod +x QUICKSTART.sh

# Start everything
./QUICKSTART.sh
```

This will:
1. Start PostgreSQL + Redis via Docker
2. Initialize database & create admin user
3. Start FastAPI backend (port 8000)
4. Start Celery worker
5. Start Celery Beat scheduler

### **Option 2: Manual Start**

**Terminal 1: Start Docker Services**
```bash
cd pipeline_orchestrator
docker-compose up
```

**Terminal 2: Initialize Database**
```bash
cd pipeline_orchestrator/backend
python init_db.py
```

**Terminal 3: Start Backend**
```bash
cd pipeline_orchestrator/backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 4: Start Celery Worker**
```bash
cd pipeline_orchestrator/backend
celery -A app.tasks.celery_app worker --loglevel=info
```

**Terminal 5: Start Celery Beat**
```bash
cd pipeline_orchestrator/backend
celery -A app.tasks.celery_app beat --loglevel=info
```

**Terminal 6: Start Frontend**
```bash
cd pipeline_orchestrator/frontend
npm install
npm run dev
```

---

## ğŸŒ Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| **Frontend UI** | http://localhost:3000 | admin@datamantri.com / admin123 |
| **Backend API** | http://localhost:8000 | Same credentials |
| **API Docs** | http://localhost:8000/api/v1/docs | N/A (Interactive) |
| **PostgreSQL** | localhost:5432 | postgres / postgres |
| **Redis** | localhost:6379 | N/A |

---

## ğŸ“‹ Complete Feature List

### **Pipeline Management**
- âœ… Create pipelines with BigQuery â†’ PostgreSQL
- âœ… Configure source (project, dataset, table, query)
- âœ… Configure destination (host, db, table, credentials)
- âœ… Set schedule (cron expressions)
- âœ… Choose mode (batch/real-time)
- âœ… View all pipelines with search & filters
- âœ… Update pipeline configuration
- âœ… Delete pipelines

### **Execution & Monitoring**
- âœ… Manual trigger (Run Now button)
- âœ… Scheduled execution (Celery Beat)
- âœ… Real-time status tracking
- âœ… Execution history with logs
- âœ… Success/failure tracking
- âœ… Detailed error messages
- âœ… Timestamp tracking

### **Authentication & Users**
- âœ… JWT-based authentication
- âœ… Login/logout functionality
- âœ… User roles (SUPER_ADMIN, ADMIN, EDITOR, VIEWER)
- âœ… Protected routes
- âœ… Session management

### **Data Transfer**
- âœ… BigQuery client integration
- âœ… PostgreSQL connection pooling
- âœ… Batch inserts (configurable size)
- âœ… Schema detection & auto-create tables
- âœ… Retry logic with exponential backoff
- âœ… Progress logging
- âœ… Error handling

---

## ğŸ¬ Usage Example

### **1. Login**
1. Go to http://localhost:3000
2. Login with `admin@datamantri.com` / `admin123`

### **2. Create Pipeline**
1. Click "Create Pipeline"
2. Fill in:
   - **Name**: "Sales Data Sync"
   - **BigQuery Source**:
     - Project: your-gcp-project
     - Dataset: sales_data
     - Table: transactions
   - **PostgreSQL Destination**:
     - Host: postgres (or localhost)
     - Database: warehouse
     - Table: sales_transactions
   - **Schedule**: `0 2 * * *` (Daily at 2 AM)
3. Click "Create Pipeline"

### **3. Run Pipeline**
1. Click "Run Now" button
2. Status changes to "Running"
3. View real-time logs
4. Check execution history

### **4. Monitor**
- Dashboard shows total/active/successful/failed pipelines
- Execution history shows all runs with logs
- Filter pipelines by status
- Search pipelines by name

---

## ğŸ“Š API Endpoints

### **Authentication**
- `POST /api/v1/auth/login` - Login
- `GET /api/v1/auth/me` - Get current user

### **Pipelines**
- `GET /api/v1/pipelines/` - List all pipelines
- `POST /api/v1/pipelines/` - Create pipeline
- `GET /api/v1/pipelines/{id}` - Get pipeline detail
- `PUT /api/v1/pipelines/{id}` - Update pipeline
- `DELETE /api/v1/pipelines/{id}` - Delete pipeline
- `POST /api/v1/pipelines/{id}/trigger` - Trigger execution
- `GET /api/v1/pipelines/{id}/runs` - Get execution history

### **Runs**
- `GET /api/v1/runs/{run_id}` - Get run detail

---

## ğŸ”§ Configuration

### **Environment Variables** (`.env` in backend/)

```env
# Database
DATABASE_URL=postgresql+psycopg2://postgres:postgres@localhost:5432/datamantri_pipelines

# JWT
SECRET_KEY=YOUR_SECRET_KEY_HERE
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Celery
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Google Cloud (for BigQuery)
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
BIGQUERY_PROJECT_ID=your-gcp-project
```

### **Google Cloud Setup**

1. Create service account in GCP
2. Download JSON key
3. Set environment variable:
   ```bash
   export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"
   ```

---

## ğŸ› Troubleshooting

### **"Port already in use"**
```bash
# Find process
lsof -i :8000  # or :3000
# Kill it
kill -9 <PID>
```

### **"Connection refused" to PostgreSQL**
```bash
# Check Docker is running
docker ps
# Restart services
docker-compose restart postgres
```

### **"No module named 'google.cloud'"**
```bash
cd backend
pip install -r requirements.txt
```

### **Frontend can't connect to backend**
- Ensure backend is running on port 8000
- Check `vite.config.ts` proxy settings
- Verify CORS in backend `main.py`

---

## ğŸ“ Database Schema

### **users**
- id, email, password_hash, role

### **pipelines**
- id, name, source_type, source_config (JSON)
- destination_type, destination_config (JSON)
- mode, schedule, status, owner_id
- created_at, updated_at

### **pipeline_runs**
- id, pipeline_id, start_time, end_time
- status, log, triggered_by

---

## ğŸ¯ Next Steps

1. âœ… **Start the system** - Run QUICKSTART.sh
2. âœ… **Login** - Use default credentials
3. âœ… **Create pipeline** - Configure your first data pipeline
4. âœ… **Test execution** - Trigger manual run
5. âœ… **Schedule it** - Set cron schedule
6. âœ… **Monitor** - Watch execution history

---

## ğŸš€ Production Deployment

### **Backend**
```bash
# Build Docker image
docker build -t pipeline-orchestrator-backend ./backend

# Run with production settings
docker run -p 8000:8000 \
  -e DATABASE_URL=postgresql://... \
  -e SECRET_KEY=... \
  pipeline-orchestrator-backend
```

### **Frontend**
```bash
cd frontend
npm run build
# Deploy dist/ folder to static hosting (Vercel, Netlify, etc.)
```

---

## ğŸ“š Documentation

- **Backend API**: http://localhost:8000/api/v1/docs
- **Frontend README**: `frontend/README.md`
- **Backend README**: `backend/README.md`

---

## ğŸŠ You're All Set!

Your complete data pipeline orchestration system is ready to use!

**Access the frontend:** http://localhost:3000

**Login:** admin@datamantri.com / admin123

**Start orchestrating your data!** ğŸš€
